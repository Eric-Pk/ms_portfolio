{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPBaO9R0E5rd0CkzcsKpkCM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","\n","# Define a simple CNN architecture\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.dropout1 = nn.Dropout(0.25)\n","        self.dropout2 = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        output = F.log_softmax(x, dim=1)\n","        return output\n","\n","# Preparing the MNIST dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n","\n","# Initialize the model, loss function, and optimizer\n","model = CNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training the model\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 100 == 0:\n","            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n","                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n","\n","# Testing the model\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += criterion(output, target).item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n","          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Start training\n","for epoch in range(1, 10 + 1):  # 10 epochs\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h79j5SfiZhnn","executionInfo":{"status":"ok","timestamp":1711760579161,"user_tz":300,"elapsed":174574,"user":{"displayName":"Silence Monk","userId":"08940594559156960137"}},"outputId":"a051108f-4e89-4b90-b0fa-b4a4c2be9c77"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307017\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.320269\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.089506\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.267362\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.088623\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.046554\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.182513\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.130726\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.045044\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.272580\n","\n","Test set: Average loss: 0.0000, Accuracy: 9844/10000 (98%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.130290\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.189991\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.062967\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.251703\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.098298\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.151975\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.099924\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.183371\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.138092\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.118098\n","\n","Test set: Average loss: 0.0000, Accuracy: 9871/10000 (99%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.095726\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.088998\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.019124\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.184662\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.041850\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.012401\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.014596\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.026070\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.072787\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.054114\n","\n","Test set: Average loss: 0.0000, Accuracy: 9897/10000 (99%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.066755\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.039788\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.068953\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.011845\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.029658\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.066473\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.069064\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.077981\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.029488\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.248431\n","\n","Test set: Average loss: 0.0000, Accuracy: 9907/10000 (99%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.089411\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.023016\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.023960\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.047374\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.036385\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.112693\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.143696\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.057004\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.016971\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.113074\n","\n","Test set: Average loss: 0.0000, Accuracy: 9911/10000 (99%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.003425\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.028517\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.020781\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.016495\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.015380\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.084541\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.006438\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.021302\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.086769\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.090224\n","\n","Test set: Average loss: 0.0000, Accuracy: 9919/10000 (99%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.016560\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.067020\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.010568\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.013466\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006041\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.154250\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.019723\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.048561\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.094163\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.031651\n","\n","Test set: Average loss: 0.0000, Accuracy: 9914/10000 (99%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.042926\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.016756\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.003155\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.034004\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.002544\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.029942\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.007789\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.006608\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.080757\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.029585\n","\n","Test set: Average loss: 0.0000, Accuracy: 9938/10000 (99%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.012706\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.040946\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.040752\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.001689\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001886\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.048980\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.006503\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.014018\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.040583\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001571\n","\n","Test set: Average loss: 0.0000, Accuracy: 9922/10000 (99%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001169\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.008676\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.020204\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.074111\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.169296\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.019478\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.019990\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.007237\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.003444\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.043612\n","\n","Test set: Average loss: 0.0000, Accuracy: 9934/10000 (99%)\n","\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlgiUfMGYzA6","executionInfo":{"status":"ok","timestamp":1711763914165,"user_tz":300,"elapsed":1115574,"user":{"displayName":"Silence Monk","userId":"08940594559156960137"}},"outputId":"06eff9fc-6c43-4fc7-95cf-058258109040"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.321165\n","\n","Test set: Average loss: 0.0020, Accuracy: 2393/10000 (24%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.987520\n","\n","Test set: Average loss: 0.0017, Accuracy: 3939/10000 (39%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.731800\n","\n","Test set: Average loss: 0.0012, Accuracy: 6491/10000 (65%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.239143\n","\n","Test set: Average loss: 0.0007, Accuracy: 8249/10000 (82%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.775068\n","\n","Test set: Average loss: 0.0004, Accuracy: 9081/10000 (91%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.453543\n","\n","Test set: Average loss: 0.0003, Accuracy: 9377/10000 (94%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.297029\n","\n","Test set: Average loss: 0.0002, Accuracy: 9484/10000 (95%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.247933\n","\n","Test set: Average loss: 0.0002, Accuracy: 9593/10000 (96%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.217970\n","\n","Test set: Average loss: 0.0002, Accuracy: 9615/10000 (96%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.158606\n","\n","Test set: Average loss: 0.0001, Accuracy: 9659/10000 (97%)\n","\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.140050\n","\n","Test set: Average loss: 0.0001, Accuracy: 9684/10000 (97%)\n","\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.118283\n","\n","Test set: Average loss: 0.0001, Accuracy: 9722/10000 (97%)\n","\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.117365\n","\n","Test set: Average loss: 0.0001, Accuracy: 9737/10000 (97%)\n","\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.094103\n","\n","Test set: Average loss: 0.0001, Accuracy: 9727/10000 (97%)\n","\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.088899\n","\n","Test set: Average loss: 0.0001, Accuracy: 9757/10000 (98%)\n","\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.082378\n","\n","Test set: Average loss: 0.0001, Accuracy: 9771/10000 (98%)\n","\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.087224\n","\n","Test set: Average loss: 0.0001, Accuracy: 9770/10000 (98%)\n","\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.057374\n","\n","Test set: Average loss: 0.0001, Accuracy: 9789/10000 (98%)\n","\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.050004\n","\n","Test set: Average loss: 0.0001, Accuracy: 9773/10000 (98%)\n","\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.077999\n","\n","Test set: Average loss: 0.0001, Accuracy: 9809/10000 (98%)\n","\n","Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.059367\n","\n","Test set: Average loss: 0.0001, Accuracy: 9807/10000 (98%)\n","\n","Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.059269\n","\n","Test set: Average loss: 0.0001, Accuracy: 9784/10000 (98%)\n","\n","Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.050428\n","\n","Test set: Average loss: 0.0001, Accuracy: 9809/10000 (98%)\n","\n","Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.047380\n","\n","Test set: Average loss: 0.0001, Accuracy: 9792/10000 (98%)\n","\n","Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.034933\n","\n","Test set: Average loss: 0.0001, Accuracy: 9804/10000 (98%)\n","\n","Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.032120\n","\n","Test set: Average loss: 0.0001, Accuracy: 9818/10000 (98%)\n","\n","Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.033209\n","\n","Test set: Average loss: 0.0001, Accuracy: 9811/10000 (98%)\n","\n","Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.030282\n","\n","Test set: Average loss: 0.0001, Accuracy: 9826/10000 (98%)\n","\n","Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.037250\n","\n","Test set: Average loss: 0.0001, Accuracy: 9829/10000 (98%)\n","\n","Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.035478\n","\n","Test set: Average loss: 0.0001, Accuracy: 9834/10000 (98%)\n","\n","Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.020446\n","\n","Test set: Average loss: 0.0001, Accuracy: 9828/10000 (98%)\n","\n","Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.017975\n","\n","Test set: Average loss: 0.0001, Accuracy: 9801/10000 (98%)\n","\n","Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.027259\n","\n","Test set: Average loss: 0.0001, Accuracy: 9825/10000 (98%)\n","\n","Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.018970\n","\n","Test set: Average loss: 0.0001, Accuracy: 9836/10000 (98%)\n","\n","Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.012473\n","\n","Test set: Average loss: 0.0001, Accuracy: 9830/10000 (98%)\n","\n","Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.023368\n","\n","Test set: Average loss: 0.0000, Accuracy: 9858/10000 (99%)\n","\n","Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.016831\n","\n","Test set: Average loss: 0.0001, Accuracy: 9833/10000 (98%)\n","\n","Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.017170\n","\n","Test set: Average loss: 0.0001, Accuracy: 9843/10000 (98%)\n","\n","Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.010899\n","\n","Test set: Average loss: 0.0001, Accuracy: 9832/10000 (98%)\n","\n","Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.014502\n","\n","Test set: Average loss: 0.0001, Accuracy: 9837/10000 (98%)\n","\n","Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.014610\n","\n","Test set: Average loss: 0.0001, Accuracy: 9824/10000 (98%)\n","\n","Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.014329\n","\n","Test set: Average loss: 0.0001, Accuracy: 9830/10000 (98%)\n","\n","Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.011317\n","\n","Test set: Average loss: 0.0001, Accuracy: 9821/10000 (98%)\n","\n","Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.017376\n","\n","Test set: Average loss: 0.0001, Accuracy: 9834/10000 (98%)\n","\n","Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.016106\n","\n","Test set: Average loss: 0.0001, Accuracy: 9839/10000 (98%)\n","\n","Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.009214\n","\n","Test set: Average loss: 0.0000, Accuracy: 9853/10000 (99%)\n","\n","Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.017692\n","\n","Test set: Average loss: 0.0001, Accuracy: 9843/10000 (98%)\n","\n","Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.020432\n","\n","Test set: Average loss: 0.0000, Accuracy: 9841/10000 (98%)\n","\n","Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.010898\n","\n","Test set: Average loss: 0.0001, Accuracy: 9825/10000 (98%)\n","\n","Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.012194\n","\n","Test set: Average loss: 0.0001, Accuracy: 9838/10000 (98%)\n","\n"]}],"source":["!pip install einops\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import einops\n","\n","# Define the Self-Attention block\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, num_heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.num_heads = num_heads\n","        self.head_dim = embed_size // num_heads\n","\n","        self.query = nn.Linear(embed_size, embed_size)\n","        self.key = nn.Linear(embed_size, embed_size)\n","        self.value = nn.Linear(embed_size, embed_size)\n","\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, embed_size = x.size()\n","\n","        q = self.query(x)\n","        k = self.key(x)\n","        v = self.value(x)\n","\n","        q = einops.rearrange(q, 'b n (h d) -> b h n d', h=self.num_heads)\n","        k = einops.rearrange(k, 'b n (h d) -> b h n d', h=self.num_heads)\n","        v = einops.rearrange(v, 'b n (h d) -> b h n d', h=self.num_heads)\n","\n","        attn_scores = torch.einsum('bhqd,bhkd->bhqk', q, k) / self.head_dim**0.5\n","        attn_probs = self.softmax(attn_scores)\n","\n","        attn_output = torch.einsum('bhqk,bhkd->bhqd', attn_probs, v)\n","        attn_output = einops.rearrange(attn_output, 'b h n d -> b n (h d)')\n","\n","        return attn_output\n","\n","# Define the Vision Transformer (ViT) architecture\n","class VisionTransformer(nn.Module):\n","    def __init__(self, patch_size=7, num_blocks=6, embed_size=64, num_heads=8, num_classes=10):\n","        super(VisionTransformer, self).__init__()\n","        self.patch_size = patch_size\n","        self.num_patches = (28 // patch_size) ** 2\n","        self.embed_size = embed_size\n","\n","        self.patch_embedding = nn.Conv2d(1, embed_size, kernel_size=patch_size, stride=patch_size)\n","\n","        self.transformer_blocks = nn.ModuleList(\n","            [nn.TransformerEncoderLayer(embed_size, num_heads) for _ in range(num_blocks)]\n","        )\n","\n","        self.fc = nn.Linear(embed_size, num_classes)\n","\n","    def forward(self, x):\n","        x = self.patch_embedding(x)\n","        x = x.flatten(2).transpose(1, 2)\n","\n","        for block in self.transformer_blocks:\n","            x = block(x)\n","\n","        x = x.mean(dim=1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","# Preparing the MNIST dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n","\n","# Initialize the model, loss function, and optimizer\n","model = VisionTransformer()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=0.001)\n","\n","# Training the model\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n","                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n","\n","# Testing the model\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += criterion(output, target).item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n","          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Start training\n","for epoch in range(1, 50 + 1):  # 10 epochs\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"]},{"cell_type":"markdown","source":["CNNs generally have stronger prior on vision tasks compared to ViTs, so it is efficient on esay task like MNIST. CNN converges faster than ViT in this example, and CNN outperforms a little bit (1% on MNIST test set) than ViT"],"metadata":{"id":"jf7E6mT4Hrfk"}}]}