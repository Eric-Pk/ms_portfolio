{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning/Pre-processing\n",
        "\n",
        "This notebook will cover the following\n",
        "\n",
        "## Text conversion\n",
        "\n",
        "    - .doc\n",
        "    - .pdf\n",
        "    - Web scraping\n",
        "    - Text cleaning"
      ],
      "metadata": {
        "id": "1y5YQtkAZyke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yDvdOohByWkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd67485-a9e6-432f-ff57-b32022868350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper\n",
        "from google.colab import drive\n",
        "\n",
        "# Below will prompt for authorization but it will make your google drive available (i.e., mount your drive).\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find out where you are and move to correct location\n",
        "import os #package for figuring out operating system\n",
        "\n",
        "os.getcwd() #what is the current working directory\n",
        "\n",
        "#os.listdir() #what is in currrent working directory\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/DS_5780_spring_25/text_cleaning\") #change directory\n",
        "\n",
        "os.listdir() #data is there"
      ],
      "metadata": {
        "id": "VqbpGmyDbNxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5b3250-ae6f-4881-93f4-3f0f321a79b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['childes.txt',\n",
              " 'Copy of syllabus.txt',\n",
              " 'annotations_cleaned.txt',\n",
              " 'syllabus.txt',\n",
              " 'boiler_plate_cleaned.txt',\n",
              " 'Crossley_DS_5780_spring_25_final.docx',\n",
              " 'annotations.txt',\n",
              " 'tweet.txt',\n",
              " 'mikolov.txt',\n",
              " 'boiler_plate.txt',\n",
              " 'misspellings_punct.txt',\n",
              " 'mikolov.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Conversion\n",
        "\n",
        "**.docx conversion**\n",
        "\n",
        "Using docx2txt\n",
        "\n",
        "  - extract text from Microsoft Word (.docx) files\n",
        "\n"
      ],
      "metadata": {
        "id": "yZltYWvQagxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n",
        "# will need to install each time because it will not be in colab library\n",
        "# docx2txt is a Python library that allows you to extract text and images\n",
        "# from Microsoft Word (.docx) files.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi4bwyp80xze",
        "outputId": "92841032-8f5d-4ea5-d7fc-0ac020147651"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt\n",
        "\n",
        "# Extract text from the .doc file\n",
        "text = docx2txt.process(\"Crossley_DS_5780_spring_25_final.docx\")\n",
        "\n",
        "print(text[:1000]) #print first 1,000 characters\n",
        "#pretty good\n",
        "\n",
        "# Save the text to a .txt\n",
        "# create file objest (f) in write mode (w) and write in the text\n",
        "with open(\"syllabus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "IbmlVdXkatF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5e6796-4ebc-4817-ae96-055fd60dc44c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DS 5780\n",
            "\n",
            "Natural Language Processing \n",
            "\n",
            "Spring 2025\n",
            "\n",
            "M/W 8:30-9:45 am, 17th & Horton A2000\n",
            "\n",
            "The course syllabus provides a general plan for the course; deviations may be necessary.\n",
            "\n",
            "\n",
            "\n",
            "1. Course information\n",
            "\n",
            "\n",
            "\n",
            "Instructor\n",
            "\n",
            "Scott Crossley\n",
            "\n",
            "Office Hours\n",
            "\n",
            "Teaching Assistants\n",
            "\n",
            "Langdon Holmes \n",
            "\n",
            "Office Hours\n",
            "\n",
            "Wesley Morris\n",
            "\n",
            "Office Hours\n",
            "\n",
            "17th & Horton B2002G\n",
            "\n",
            "Email: scott.crossley@vanderbilt.edu\n",
            "\n",
            "Monday, 10-11 am, virtual office hours on Zoom by request\n",
            "\n",
            "17th & Horton B2002G\n",
            "\n",
            "Email: langdon.holmes@Vanderbilt.Edu\n",
            "\n",
            "Tuesday, 12-1 pm, virtual office hours on Zoom by request\n",
            "\n",
            "Email: wesley.g.morris@Vanderbilt.Edu\n",
            "\n",
            "Wednesday, 10-11 am, virtual office hours on Zoom by request\n",
            "\n",
            "\n",
            "\n",
            "2. Course description\n",
            "\n",
            "\n",
            "\n",
            "This course is colloquially titled “from tokens to transformers” and will focus on using computers to automatically analyze language data for linguistic features. The goal of the course is to provide students with the background and computing skills necessary to independently analyze and assess languag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**.pdf conversion**\n",
        "\n",
        "Using PyMuPDF"
      ],
      "metadata": {
        "id": "iT-tZhoHatt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need to install each time as well\n",
        "!pip install PyMuPDF\n",
        "# allows Python to work with PDF files\n",
        "# reading and extracting information\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP2qvdeX3ZGM",
        "outputId": "efb31694-213d-4757-feab-cbb82625e7a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import fitz  # From PyMuPDF\n",
        "# fitz is the main module you interact with to work with PDF files\n",
        "\n",
        "# Open the PDF of the original BERT paper\n",
        "with fitz.open(\"mikolov.pdf\") as doc: # with allows for immediate close of document\n",
        "    # Extract text from all pages\n",
        "    text = \"\" #empty string variable\n",
        "    for page in doc: #goes through each page in doc\n",
        "        text += page.get_text() #extracts info\n",
        "\n",
        "\n",
        "# Print the text\n",
        "print(text[:2000]) #print first 2,000 characters\n",
        "\n",
        "# Save the text to a .txt\n",
        "with open(\"mikolov.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "RE4llwbAawzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87acf3d4-d673-41ee-ca0b-71f58197855b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distributed Representations of Words and Phrases\n",
            "and their Compositionality\n",
            "Tomas Mikolov\n",
            "Google Inc.\n",
            "Mountain View\n",
            "mikolov@google.com\n",
            "Ilya Sutskever\n",
            "Google Inc.\n",
            "Mountain View\n",
            "ilyasu@google.com\n",
            "Kai Chen\n",
            "Google Inc.\n",
            "Mountain View\n",
            "kai@google.com\n",
            "Greg Corrado\n",
            "Google Inc.\n",
            "Mountain View\n",
            "gcorrado@google.com\n",
            "Jeffrey Dean\n",
            "Google Inc.\n",
            "Mountain View\n",
            "jeff@google.com\n",
            "Abstract\n",
            "The recently introduced continuous Skip-gram model is an efﬁcient method for\n",
            "learning high-quality distributed vector representations that capture a large num-\n",
            "ber of precise syntactic and semantic word relationships. In this paper we present\n",
            "several extensions that improve both the quality of the vectors and the training\n",
            "speed. By subsampling of the frequent words we obtain signiﬁcant speedup and\n",
            "also learn more regular word representations. We also describe a simple alterna-\n",
            "tive to the hierarchical softmax called negative sampling.\n",
            "An inherent limitation of word representations is their indifference to word order\n",
            "and their inability to represent idiomatic phrases. For example, the meanings of\n",
            "“Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated\n",
            "by this example, we present a simple method for ﬁnding phrases in text, and show\n",
            "that learning good vector representations for millions of phrases is possible.\n",
            "1\n",
            "Introduction\n",
            "Distributed representations of words in a vector space help learning algorithms to achieve better\n",
            "performance in natural language processing tasks by grouping similar words. One of the earliest use\n",
            "of word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea\n",
            "has since been applied to statistical language modeling with considerable success [1]. The follow\n",
            "up work includes applications to automatic speech recognition and machine translation [14, 7], and\n",
            "a wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].\n",
            "Recently, Mikolov et al. [8] introduced the Skip-gram model, an efﬁcient method for learning high-\n",
            "quality vector representation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web scraping**\n",
        "\n",
        "Using BeautifulSoup\n",
        "\n",
        "What it does\n",
        "\n",
        "    - Makes a GET Request and stores the response\n",
        "    - Check status code (typically 200 for success)\n",
        "      - i.e., can you access the data\n",
        "    - Parse the HTML and stores data\n",
        "    - Print the Prettified HTML\n",
        "      - It's not pretty\n",
        "\n"
      ],
      "metadata": {
        "id": "k_DzzjHNaxRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install beautifulsoup4\n",
        "# should be installed in colab as a base package"
      ],
      "metadata": {
        "id": "DY_OfAYDa0sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Making a GET request and store variable as r\n",
        "r = requests.get('https://www.bbc.com/news/articles/cm2enepy8g8o')\n",
        "\n",
        "# Is data available\n",
        "# Success code should = 200\n",
        "print(\"Response: \", r)\n",
        "\n",
        "# Parse the html using html.parser and store in soup\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBS7sxcF7flK",
        "outputId": "7f89bbd9-b065-46fb-8f2b-8a25652d4f7f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:  <Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**\n",
        "\n",
        "The researcher needs to identify all the tags to remove\n",
        "  - This can be a tedious process\n",
        "  - Below is an example of how it can be done (imperfectly)"
      ],
      "metadata": {
        "id": "GeBMwvIFomel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.prettify()[:5000]) #print the first 1,000 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ215WgjodG4",
        "outputId": "815fbe10-96a3-4644-82f4-d9db5984878b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en-GB\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
            "  <title>\n",
            "   Dick Van Dyke and Cher forced to evacuate Franklin fire in Malibu\n",
            "  </title>\n",
            "  <meta content=\"Dick Van Dyke and Cher forced to evacuate Franklin fire in Malibu\" property=\"og:title\"/>\n",
            "  <meta content=\"Dick Van Dyke and Cher forced to evacuate Franklin fire in Malibu\" name=\"twitter:title\"/>\n",
            "  <meta content=\"The blaze has burned more than 4,000 acres in the celebrity enclave near Los Angeles.\" name=\"description\"/>\n",
            "  <meta content=\"The blaze has burned more than 4,000 acres in the celebrity enclave near Los Angeles.\" property=\"og:description\"/>\n",
            "  <meta content=\"The blaze has burned more than 4,000 acres in the celebrity enclave near Los Angeles.\" name=\"twitter:description\"/>\n",
            "  <meta content=\"https://ichef.bbci.co.uk/news/1024/branded_news/e482/live/9202b190-b7ad-11ef-aff0-072ce821b6ab.jpg\" property=\"og:image\"/>\n",
            "  <meta content=\"https://ichef.bbci.co.uk/news/1024/branded_news/e482/live/9202b190-b7ad-11ef-aff0-072ce821b6ab.jpg\" name=\"twitter:image:src\"/>\n",
            "  <meta content=\"summary_large_image\" name=\"twitter:card\"/>\n",
            "  <meta content=\"A firefighter stands in front of a large blaze\" property=\"og:image:alt\"/>\n",
            "  <meta content=\"A firefighter stands in front of a large blaze\" name=\"twitter:image:alt\"/>\n",
            "  <meta content=\"#da532c\" name=\"msapplication-TileColor\"/>\n",
            "  <meta content=\"#ffffff\" name=\"theme-color\"/>\n",
            "  <meta content=\"NOODP, NOYDIR\" name=\"robots\"/>\n",
            "  <meta content=\"app-id=364147881, app-argument=https://www.bbc.com/news/articles/cm2enepy8g8o\" name=\"apple-itunes-app\"/>\n",
            "  <link href=\"https://www.bbc.com/news/articles/cm2enepy8g8o.amp\" rel=\"amphtml\"/>\n",
            "  <link href=\"/bbcx/apple-touch-icon.png\" rel=\"apple-touch-icon\" sizes=\"180x180\"/>\n",
            "  <link href=\"/bbcx/favicon-32x32.png\" rel=\"icon\" sizes=\"32x32\" type=\"image/png\"/>\n",
            "  <link href=\"/bbcx/favicon-16x16.png\" rel=\"icon\" sizes=\"16x16\" type=\"image/png\"/>\n",
            "  <link href=\"/bbcx/favicon.ico\" rel=\"alternate icon\"/>\n",
            "  <link href=\"/bbcx/site.webmanifest\" rel=\"manifest\"/>\n",
            "  <link color=\"#000000\" href=\"/bbcx/safari-pinned-tab.svg\" rel=\"mask-icon\"/>\n",
            "  <link href=\"https://www.bbc.com/news/articles/cm2enepy8g8o\" rel=\"canonical\"/>\n",
            "  <link data-testid=\"en-hreflang-tag\" href=\"https://www.bbc.com/news/articles/cm2enepy8g8o\" hreflang=\"en\" rel=\"alternate\"/>\n",
            "  <link data-testid=\"en-gb-hreflang-tag\" href=\"https://www.bbc.co.uk/news/articles/cm2enepy8g8o\" hreflang=\"en-gb\" rel=\"alternate\"/>\n",
            "  <meta content=\"2.14.1+2\" name=\"version\"/>\n",
            "  <script type=\"application/ld+json\">\n",
            "   {\"@context\":\"http://schema.org\",\"@type\":\"ReportageNewsArticle\",\"url\":\"https://www.bbc.com/news/articles/cm2enepy8g8o\",\"publisher\":{\"@type\":\"NewsMediaOrganization\",\"name\":\"BBC News\",\"publishingPrinciples\":\"http://www.bbc.co.uk/news/help-41670342\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https://static.files.bbci.co.uk/ws/simorgh-assets/public/news/images/metadata/poster-1024x576.png\"}},\"datePublished\":\"2024-12-10T10:01:35.184Z\",\"dateModified\":\"2024-12-13T00:21:05.427Z\",\"description\":\"The blaze has burned more than 4,000 acres in the celebrity enclave near Los Angeles.\",\"headline\":\"Dick Van Dyke and Cher forced to evacuate Franklin fire in Malibu\",\"image\":{\"@type\":\"ImageObject\",\"width\":1024,\"height\":576,\"url\":\"https://ichef.bbci.co.uk/news/1024/branded_news/e482/live/9202b190-b7ad-11ef-aff0-072ce821b6ab.jpg\"},\"thumbnailUrl\":\"https://ichef.bbci.co.uk/news/1024/branded_news/e482/live/9202b190-b7ad-11ef-aff0-072ce821b6ab.jpg\",\"mainEntityOfPage\":\"https://www.bbc.com/news/articles/cm2enepy8g8o\",\"author\":[{\"@type\":\"Person\",\"name\":\"Christal Hayes\"}]}\n",
            "  </script>\n",
            "  <meta content=\"bbcx://news/articles/cm2enepy8g8o\" property=\"al:ios:url\"/>\n",
            "  <meta content=\"364147881\" property=\"al:ios:app_store_id\"/>\n",
            "  <meta content=\"BBC: World News &amp; Stories\" property=\"al:ios:app_name\"/>\n",
            "  <meta content=\"bbcx://news/articles/cm2enepy8g8o\" property=\"al:android:url\"/>\n",
            "  <meta content=\"BBC: World News &amp; Stories\" property=\"al:android:app_name\"/>\n",
            "  <meta content=\"bbc.mobile.news.ww\" property=\"al:android:package\"/>\n",
            "  <meta content=\"https://bbc.com/news/articles/cm2enepy8g8o\" property=\"al:web:url\"/>\n",
            "  <meta content=\"36\" name=\"next-head-count\"/>\n",
            "  <script async=\"\" data-nscript=\"beforeInteractive\" id=\"inline-dotcom\">\n",
            "   window.dotcom = window.dotcom || { cmd: [] };\n",
            "  window.dotcom.ads = window.dotcom.ads || {\n",
            "   resolves: {enabled: [], getAdTag: []},\n",
            "   enabled: () => new Promise(r => window.dotcom.ads.resolves.enabled.push(r)),\n",
            "   getAdTag: () => new Promise(r => window.dotcom.ads.resolves.getAdTag.push(r))\n",
            "  };\n",
            "  setTimeout(() => {\n",
            "    if(window.dotcom.ads.resolves){\n",
            "      window.dotcom.ads.resolves.enabled.forEach(r => r(false));\n",
            "      window.dotcom.ads.resolves.getAdTag.forEach(r => r(\"\"));\n",
            "      window.dotcom.ads.enabled = () => new Promise(r => r(false));\n",
            "      window.dotcom.ads.getAdTag = () => new Promise(r => r(\"\"));\n",
            "      console.error(\"NGAS load timeout\");\n",
            "    }\n",
            "  }, 5000)\n",
            "  </script>\n",
            "  <script async=\"\" data-nscript=\"be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove html tags\n",
        "# This is not perfect\n",
        "\n",
        "#clean up data by removing all the html tags found\n",
        "for data in soup(['noscript','style', 'script']): #select the style and script tags\n",
        "  data.decompose() # and remove those tags\n",
        "\n",
        "# Get the text content\n",
        "text = ' '.join(soup.stripped_strings) # returns a generator of all the strings\n",
        "# in the parsed data with whitespace stripped\n",
        "\n",
        "print(text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIEp0sFL7fUl",
        "outputId": "f79be8a4-7f3c-4735-8e9e-860f2b283cc7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dick Van Dyke and Cher forced to evacuate Franklin fire in Malibu Skip to content British Broadcasting Corporation Watch Live Home News Sport Business Innovation Culture Arts Travel Earth Video Live Home News Sport Business Innovation Culture Arts Travel Earth Video Live Audio Weather Newsletters Celebrities among thousands fleeing Malibu wildfire 13 December 2024 Christal Hayes BBC News Reporting from Los Angeles, California Watch: Malibu residents under ‘blowtorch’ of growing Franklin Fire Actors Dick Van Dyke and Mark Hamill and musician Cher are among thousands of residents who have been told to leave their homes in the city of Malibu after a fast-moving wildfire broke out and burned acres of the wealthy enclave near Los Angeles. Hundreds of personnel have been deployed to fight the Franklin Fire, which started late on Monday in Malibu Canyon before torching several homes. No injuries have been reported, and while the blaze has grown it was 20% contained as of Wednesday. The cause is not yet clear, but the region has been under a red-flag warning, meaning conditions are ripe for extreme fires. More than 4,000 acres of land have been burned, according to officials' latest information. Some 2,000 firefighters have been sent to tackle the flames, which have entered steep terrain making it harder for crews to access. Much of the city has been under mandatory evacuation order and federal assistance has been made available, Governor Gavin Newsom said. Smoke advisories have been issued for surrounding areas, with people advised to limit or avoid outdoor activities. The blaze has left a trail of burnt-out cars and buses as well as the charred remains of homes. Nine structures were destroyed including four homes, authorities said. Malibu is popular with celebrities. Homes owned by stars including Lady Gaga, Beyonce and Jay Z were reportedly among those in the evacuation zone. Van Dyke was among the stars forced to flee, he said in a post on Facebook. The Mary Poppins star, who turns 99 on Friday, said he and his wife Arlene safely left with their pets, except for one cat named Bobo, who was missing for several hours. \"We found Bobo as soon as we arrived back home this morning,\" Van Dyke wrote on Facebook. \"There was so much interest in his disappearance that Animal Control was called in to assist. But, thankfully, he was easy to find and not harmed.\" Witnesses told KABC-TV that the home belonging to the actor's neighbour had caught fire, but that Dyke's was untouched. Singer Cher also had to flee the fire, according to the New York Times. A representative told Fox that she had yet to return home. Barbra Streisand is another famous resident of Malibu, however, her publicist told the New York Times it was unclear whether she had evacuated. Star Wars actor Mark Hamill told fans on Instagram that he was \"in lockdown\" because of the fires and remained in his home. \"I'm not allowed to leave the house, which fits in with my elderly-recluse lifestyle,\" he wrote. Other celebrities who confirmed they were impacted by the blaze were ex-Dancing with the Stars champion and TV personality Brook Burke, British actress Jane Seymour and Academy Award winner Mira Sorvino. Getty Images The remnants of a home in Malibu after the blaze roared through the area Wildfires more generally in California have the capacity to burn through tens of thousands of acres of vegetation due to the typical dry conditions in the region. The Franklin Fire is relatively small by comparison, though it has spread quickly. Schools in the area have closed, roads are shut, and power has been cut locally to prevent worsening the blaze. Tens of thousands of homes were disconnected on Wednesday morning, according to tracking site Poweroutage.us. The vast majority were reconnected by Thursday afternoon. Meanwhile, evacuation centres have been opened for residents and animals. More than 5,000 people were in the evacuation zone, according to data from the California Department of Forestry and Fire Protection (Cal Fire). The blaze broke out near Pepperdine University. The university initially directed students to shelter in place, but has since lifted that order and said fire activity nearby had \"greatly diminished\". Footage showed students sheltering in the university library as a wall of fire approached on Monday evening. Fire crews hosed down the flames. Pepperdine reported there was minor damage on campus, but no-one was hurt. Pepperdine students shelter in library as Malibu fire nears campus Officials have warned that this blaze has been fuelled by the seasonal Santa Ana winds. It started about three miles (4.8km) north of the Pacific Coast Highway (PCH) - an iconic road along the Pacific Ocean known for its stunning views - and quickly spread south, jumping across the route into the Malibu Pier area. The evacuation order covered a region that included east of Malibu Canyon Road and south of Puma Road as well as the Serra Retreat area, the County of Los Angeles Fire Department said. A map of the blaze shows it was bordering city hall government facilities, a school and a number of homes in the area - including a line of oceanfront properties. The latest incident comes about a month after another fire forced thousands of people to evacuate another nearby city, Moorpark. California is a state that is prone to wildfires. The amount of burned areas in the summer in northern and central part of the state increased fivefold from 1996 to 2021 compared with the 24-year period before. Scientists have attributed this to climate change, though not all wildfires can automatically be linked directly to this cause. The science is complicated and human factors, including how we manage land and forests, also contribute. However, scientists say climate change is making weather conditions that lead to wildfires, such as heat and drought, more likely. Have you been asked to evacuate because of the fire? Get in touch. California wildfire forces thousands to evacuate Four ways climate change worsens extreme weather California wildfires United States Related Trump tours LA fire destruction amid worries about disaster aid 2 days ago US & Canada Even before the LA fires, Californians fled for 'climate havens' 3 days ago US & Canada Firefighters battle huge blaze near Los Angeles as winds pick up 3 days ago US & Canada More 2 hrs ago Trump ally criticises pardons for violent Jan 6 offenders Republican Lindsey Graham said it was a mistake to pardon anyone who \"beat up a police officer\" at US Capitol. 2 hrs ago World 3 hrs ago Co-operate or else: Trump's Colombia face-off is warning to all leaders But there are risks too - coffee prices could rise and nations may be less willing to stop the flow of migrants north. 3 hrs ago World 3 hrs ago Trump imposes 25% tariffs on Colombia as deported migrant flights blocked It comes after Colombian President Gustavo Petro denied entry to US military planes carrying deported migrants. 3 hrs ago World 23 hrs ago Trump says he believes US will 'get Greenland' The president told reporters the island's 57,000 residents \"want to be with us\". 23 hrs ago World 24 hrs ago Trump says Keir Starmer doing 'very good job' The US president tells BBC he feels the UK prime minister has \"done a very good job thus far\". 24 hrs ago US & Canada British Broadcasting Corporation Home News Sport Business Innovation Culture Arts Travel Earth Video Live Audio Weather BBC Shop BBC in other languages Follow BBC on: Terms of Use About the BBC Privacy Policy Cookies Accessibility Help Contact the BBC Advertise with us Do not share or sell my info Contact technical support Copyright 2025 BBC. All rights reserved. The BBC is not responsible for the content of external sites. Read about our approach to external linking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning\n",
        "\n",
        "Work on the removal of\n",
        "  - Removing XML\n",
        "  - Other meta-data removal\n",
        "  - Non-alphabetic/numeric characters\n",
        "  - Punctuation and white space problems\n",
        "  - Misspellings\n"
      ],
      "metadata": {
        "id": "UAvk1ep1VQiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XML tags\n",
        "\n",
        "A simple linguistic annotation example is provided below\n",
        "  - Grammar and mechanical error tags\n",
        "\n",
        "If you are cleaning an XML text that is hierarchical in nature (like the BNC), it is probably better to use an XML parsing library\n",
        "  - like lxml\n",
        "  - or use much more complicated code\n",
        "\n"
      ],
      "metadata": {
        "id": "AtF_Vft94_0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing XML tags\n",
        "\n",
        "import re\n",
        "\n",
        "with open('annotations.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text) #this is a text with simple XML for grammar and spelling errors\n",
        "pattern = r'<.*?>'  #regex pattern (non-greedy)\n",
        "# matches any character (.) zero or more times (*) as few times as possible (?)\n",
        "\n",
        "cleaned_text = re.sub(pattern, '', text)\n",
        "cleaned_text = re.sub(\" +\", \" \", cleaned_text) #get rid of extra space\n",
        "\n",
        "print(cleaned_text) #this is a text without XML\n",
        "\n",
        "with open('annotations_cleaned.txt', 'w') as file:\n",
        "    file.write(cleaned_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOakkPgj9inQ",
        "outputId": "fbbbd354-dd10-4046-80fa-66a5cf7a89ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Many people asks <GVN> themselves Weather <FSH> <FSC> its <PM> better to work <WM> cooperating with one another or weather <FSH> <FSC> its <PG> better to be always competing. I think <WM> both ways you can achieve success because when you compete with one another <PM> you try more why <WR> because you really want to achieve that goal. And <PM> when cooperate <GVM> with one another <PM> you can achieve great things aswell <FS> because more hands are better than two <PM> dont <PM> they say. but <FSC> <PM> here are some of the reaons <FS> why i <FSC> thing <FS> cooperation helps you achieve more success than competition.\n",
            "One of the reasons why i <FSC> believe cooperation achieves you <LP> more success than competing is because <PM> when you cooperate with someone <PM> <WM> means you help each other out, so <PM> like they say <PM> two heads are better than one. And <PM> when you compete <PM> you might be on your own and competition doesn't get you where you would like to be at <WM> points <GNN> . I also think <PM> yes, competition is everywhere <PM> and <PM> sometimes <PM> you have to compete to get to where you want to be. But <PM> competition gains you a lot of stress <PM> and <WM> <WM> to do things <WM> maybe you know you shouldn't do. I know it feels good to have someone help you <PM> and <PM> sometimes <PM> you you <WR> would like to do things on your own <PM> but i've <FSC> seen the biggest companies cooperating to get this <AR> done and that's when things come out better.\n",
            "Another reason i <FSC> have is that cooperations <GNN> is likely to produce lasting accomplishments <PM> that has been proven. And <PM> what do you gain competing <PM> yeah, important victories. But <PM> i <FSC> <WM> rather have good quality work, <PW> with people making sure what i'm <FSC> doing is correct and he <SF>\n",
            "\n",
            "\n",
            "\n",
            "Many people asks themselves Weather its better to work cooperating with one another or weather its better to be always competing. I think both ways you can achieve success because when you compete with one another you try more why because you really want to achieve that goal. And when cooperate with one another you can achieve great things aswell because more hands are better than two dont they say. but here are some of the reaons why i thing cooperation helps you achieve more success than competition.\n",
            "One of the reasons why i believe cooperation achieves you more success than competing is because when you cooperate with someone means you help each other out, so like they say two heads are better than one. And when you compete you might be on your own and competition doesn't get you where you would like to be at points . I also think yes, competition is everywhere and sometimes you have to compete to get to where you want to be. But competition gains you a lot of stress and to do things maybe you know you shouldn't do. I know it feels good to have someone help you and sometimes you you would like to do things on your own but i've seen the biggest companies cooperating to get this done and that's when things come out better.\n",
            "Another reason i have is that cooperations is likely to produce lasting accomplishments that has been proven. And what do you gain competing yeah, important victories. But i rather have good quality work, with people making sure what i'm doing is correct and he \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**See Appendix 1 at end of notebook**\n",
        "\n",
        "If you are interested in constructing a map of character indexes between two string versions\n",
        "  - i.e., a map that includes the word and the annotation"
      ],
      "metadata": {
        "id": "ld21CpXMvPIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadata\n",
        "\n",
        "This data comes from the Switchboard Corpus\n",
        "\n",
        "- Switchboard is a collection of about 2,400 two-sided telephone conversations among 543 speakers (302 male, 241 female) from all areas of the United States\n",
        "\n",
        "Data has some unique attributes\n",
        "- Introduction section\n",
        "- Two different speakers\n",
        "- Meta-linguistic data [laughter]\n",
        "- False starts\n",
        "  - right now I-, I'm not...\n",
        "- Uncertain translations\n",
        "  - ((Well, let's see))\n",
        "- Hashtags (no idea what they represent)"
      ],
      "metadata": {
        "id": "_t8nD99u5EHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove metadata from Switchboard corpus\n",
        "\n",
        "with open('boiler_plate.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:2000]) #this is the text we need to clean\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzULV-u8lXDn",
        "outputId": "917783c8-f6ba-4c57-e3e6-4c57291196d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FILENAME:\t2001_1020_1044\n",
            "TOPIC#:\t\t303\n",
            "DATE:\t\t910304\n",
            "TRANSCRIBER:\tkhb\n",
            "DIFFICULTY:\t1\n",
            "TOPICALITY:\t1\n",
            "NATURALNESS:\t2\n",
            "ECHO_FROM_B:\t1\n",
            "ECHO_FROM_A:\t3\n",
            "STATIC_ON_A:\t1\n",
            "STATIC_ON_B:\t1\n",
            "BACKGROUND_A:\t1\n",
            "BACKGROUND_B:\t1\n",
            "REMARKS:\n",
            "\n",
            "==========================================================================\n",
            "\n",
            "B.1:  Okay.\n",
            "\n",
            "A.2:  Hi.\n",
            "\n",
            "B.3:  Hi.\n",
            "\n",
            "A.4:  Um, yeah, I would like to talk about how you dress for work, and, and, um,\n",
            "what do you normally, what type of outfit do you normally have to wear?\n",
            "\n",
            "B.5:  Well, I work in, uh, Corporate Control, so we have to dress kind of nice,\n",
            "so I usually wear skirts and sweaters in the winter time, slacks, I guess.\n",
            "\n",
            "A.6:  Uh-huh.\n",
            "\n",
            "B.7:  And in the summer, just dresses.  We can't even, well, we're not even\n",
            "really supposed to wear jeans very often, so,\n",
            "\n",
            "A.8:  And is,\n",
            "\n",
            "B.9:  It really doesn't vary that much from season to season since the office is\n",
            "kind of, you know, always the same temperature.\n",
            "\n",
            "A.10:  Right, right.  Is there, is there, um, a-, is there a, like a code of dress\n",
            "where you work?  Do they ask,\n",
            "\n",
            "B.11:  Not formally, \n",
            "\n",
            "A.12:  Right.\n",
            "\n",
            "B.13:  but it's kind of understood that we're supposed to dress a little bit\n",
            "nice.  A lot of times we have to go over to, uh, like Jerry Junkins' office\n",
            "and Bill Ellsworth's office to deliver stuff,\n",
            "\n",
            "A.14:  Right.\n",
            "\n",
            "B.15:  and we prepare a lot of foils for Marvin, and Bill, so we have to dress a\n",
            "little bit nice. We're not,\n",
            "\n",
            "A.16:  Right, right.  And does it, does it change?  I guess, um, you can, can you\n",
            "judge like, uh, depending on what you think you'll be doing that day?\n",
            "\n",
            "B.17:  Yes, now, you know, if, if every-, like in August when everybody is on\n",
            "vacation or something, we can dress a little more casual,\n",
            "\n",
            "A.18:  Right.\n",
            "\n",
            "B.19:  or, ice storms, of course, you know, we all came in in our tennis shoes\n",
            "but, \n",
            "\n",
            "A.20:  Yes [laughter].\n",
            "\n",
            "B.21:  [Laughter] I guess that would have to do with the weather.\n",
            "\n",
            "A.22:  Yeah, #yeah, well i-, it's,#\n",
            "\n",
            "B.23:  #But, um,#\n",
            "\n",
            "A.24:  that's right.  #And it,#\n",
            "\n",
            "B.25:  #unless# it's an ex-, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re # we will need RegEx here\n",
        "\n",
        "# get rid of intro\n",
        "pattern_intro = r'(FILENAME:.*?REMARKS:\\s*\\n)?={2,}'\n",
        "# () starts a grouping that matches FILENAME:\n",
        "# followed by any characters (.) zero of more times (*) non-greedily (?) so it matches\n",
        "# as little as possible until it reaches \"REMARKS:\n",
        "# \\s*\\n: equals optional whitespace (\\s*), and a newline character (\\n)\n",
        "# next part gets rid of ====, ? zero of more occurrences of = that happens at least twice {2,}\n",
        "\n",
        "# get rid of data within parentheses and square brackets\n",
        "pattern_meta = r'\\([^)]*\\)|\\[[^]]*\\]|#|-'\n",
        "# Find \\( and \\). \\ is used as an escape character. It indicates that the character\n",
        "# following it should be treated specially. [] are used to define a character class.\n",
        "# A character class matches any one of the characters inside the brackets.\n",
        "# [^)] is a character class that matches any character except the closing parenthesis ).\n",
        "# * is a  quantifier that means \"zero or more occurrences\"\n",
        "# | is or, so we add in anything in between [], the hashtag, or --\n",
        "\n",
        "cleaned_text = re.sub(pattern_intro, '', text, flags=re.DOTALL)\n",
        "# re.DOTALL makes the dot (.) match any character, including newline characters\n",
        "# so we can search for patterns across multiple lines and line breaks\n",
        "# which is what happens with the introduction\n",
        "\n",
        "cleaned_text_2 = re.sub(pattern_meta, '', cleaned_text)\n",
        "'''\n",
        "“cleaned_text = re.sub (pattern_intro, '', text, flags=re.DOTALL)” 是一段 Python 代码语句。\n",
        "它使用正则表达式模块（re）的 sub 方法。这个方法的作用是将文本（text）中的与给定正则表达式模式（pattern_intro）匹配的部分替换为空字符串（''），并将替换后的结果赋值给变量 cleaned_text。参数 flags=re.DOTALL 表示让正则表达式中的点号（.）可以匹配包括换行符在内的任意字符。\n",
        "'''\n",
        "print(cleaned_text_2[:2000]) #this is a text without XML\n",
        "with open('boiler_plate_cleaned.txt', 'w') as file:\n",
        "    file.write(cleaned_text_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKig2qhZwBgz",
        "outputId": "552b46d2-57e7-45ac-8198-9e17ee9156ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "B.1:  Okay.\n",
            "\n",
            "A.2:  Hi.\n",
            "\n",
            "B.3:  Hi.\n",
            "\n",
            "A.4:  Um, yeah, I would like to talk about how you dress for work, and, and, um,\n",
            "what do you normally, what type of outfit do you normally have to wear?\n",
            "\n",
            "B.5:  Well, I work in, uh, Corporate Control, so we have to dress kind of nice,\n",
            "so I usually wear skirts and sweaters in the winter time, slacks, I guess.\n",
            "\n",
            "A.6:  Uhhuh.\n",
            "\n",
            "B.7:  And in the summer, just dresses.  We can't even, well, we're not even\n",
            "really supposed to wear jeans very often, so,\n",
            "\n",
            "A.8:  And is,\n",
            "\n",
            "B.9:  It really doesn't vary that much from season to season since the office is\n",
            "kind of, you know, always the same temperature.\n",
            "\n",
            "A.10:  Right, right.  Is there, is there, um, a, is there a, like a code of dress\n",
            "where you work?  Do they ask,\n",
            "\n",
            "B.11:  Not formally, \n",
            "\n",
            "A.12:  Right.\n",
            "\n",
            "B.13:  but it's kind of understood that we're supposed to dress a little bit\n",
            "nice.  A lot of times we have to go over to, uh, like Jerry Junkins' office\n",
            "and Bill Ellsworth's office to deliver stuff,\n",
            "\n",
            "A.14:  Right.\n",
            "\n",
            "B.15:  and we prepare a lot of foils for Marvin, and Bill, so we have to dress a\n",
            "little bit nice. We're not,\n",
            "\n",
            "A.16:  Right, right.  And does it, does it change?  I guess, um, you can, can you\n",
            "judge like, uh, depending on what you think you'll be doing that day?\n",
            "\n",
            "B.17:  Yes, now, you know, if, if every, like in August when everybody is on\n",
            "vacation or something, we can dress a little more casual,\n",
            "\n",
            "A.18:  Right.\n",
            "\n",
            "B.19:  or, ice storms, of course, you know, we all came in in our tennis shoes\n",
            "but, \n",
            "\n",
            "A.20:  Yes .\n",
            "\n",
            "B.21:   I guess that would have to do with the weather.\n",
            "\n",
            "A.22:  Yeah, yeah, well i, it's,\n",
            "\n",
            "B.23:  But, um,\n",
            "\n",
            "A.24:  that's right.  And it,\n",
            "\n",
            "B.25:  unless it's an ex, you know, an unusual day,\n",
            "\n",
            "A.26:  Yeah, and it was usually, uh, also, uh, where I was, it was, um, i, in\n",
            "engineering, and,\n",
            "\n",
            "B.27:  Uhhuh.\n",
            "\n",
            "A.28:  and it was, it was a casual office, there was no formal dress code, but,\n",
            "um, uh, usually engineers are expected to, to dress, you know, a little more\n",
            "professionally, it,\n",
            "\n",
            "B.29:  Ye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What if you only want data by individual?\n",
        "\n",
        "person_A = re.findall(r\"^A.\\d+:\\s.*\", cleaned_text_2, re.MULTILINE)\n",
        "# ^ beginning of line followed by A followed by any character (.) followed by\n",
        "# any digit (\\d+) followed by a literal colon and then whitespace (\\s)\n",
        "# and any character zero or more captures the words after the demarker\n",
        "# re.MULTILINE across lines of text\n",
        "\n",
        "for line in person_A:\n",
        "    print(line) #did we get them?\n",
        "\n",
        "person_A_text = ' '.join(person_A) #join all the lines together\n",
        "\n",
        "print(person_A_text[:1000]) #print it out\n",
        "\n",
        "pattern_A = r'(A.*?:)' #regex to remove demarker\n",
        "\n",
        "person_A_cleaned = re.sub(pattern_A, '', person_A_text)\n",
        "\n",
        "print(person_A_cleaned[:1000]) #the cleaned text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeR6HDWgEOSF",
        "outputId": "49dd022b-bf18-4bcb-a2eb-1648226a0846"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A.2:  Hi.\n",
            "A.4:  Um, yeah, I would like to talk about how you dress for work, and, and, um,\n",
            "A.6:  Uhhuh.\n",
            "A.8:  And is,\n",
            "A.10:  Right, right.  Is there, is there, um, a, is there a, like a code of dress\n",
            "A.12:  Right.\n",
            "A.14:  Right.\n",
            "A.16:  Right, right.  And does it, does it change?  I guess, um, you can, can you\n",
            "A.18:  Right.\n",
            "A.20:  Yes .\n",
            "A.22:  Yeah, yeah, well i, it's,\n",
            "A.24:  that's right.  And it,\n",
            "A.26:  Yeah, and it was usually, uh, also, uh, where I was, it was, um, i, in\n",
            "A.28:  and it was, it was a casual office, there was no formal dress code, but,\n",
            "A.30:  just in case things would come up during the day, \n",
            "A.32:  sometimes unexpected meetings or a client would come in and would want to\n",
            "A.34:  uh, other times it could be very casual.  If you knew you would be at a\n",
            "A.36:  Um.\n",
            "A.38:  Uh, well, I was, I was in a, uh, private consulting firm, \n",
            "A.40:  so, um, and, uh,\n",
            "A.42:  and, uh, anyway, um, right now I, I'm not, I'm not there but ,\n",
            "A.44:  but, anyway, um, and seas, really same, season to season, um, uh, it\n",
            "A.46:  weather, you know,\n",
            "A.48:  wear light clothing I guess, if it's hot out, but, um,\n",
            "A.50:  that's \n",
            "A.52:   that's true, and, and I think also that, um, the women actually have a,\n",
            "A.54:  and woolens in the winter, where as the, the men often,\n",
            "A.56:  have to wear shirt and tie  no matter,\n",
            "A.58:  Right, right. What time of, what time of the year.\n",
            "A.60:  That's right.\n",
            "A.62:  Tha,  That's, that's true, so, um,\n",
            "A.64:  do they, were, were there, um, are you allowed to, um, be casual, like if\n",
            "A.66:  Right.\n",
            "A.68:  Oh, I see, uhhuh.\n",
            "A.70:  Okay, and, and \n",
            "A.72:   and also, like the, um, also the, the, all the supporting kind of stuff,\n",
            "A.74:  they have same, similar?\n",
            "A.76:  Yeah.\n",
            "A.78:  Yeah, ours were, ours were different.  I guess it seemed like th, they\n",
            "A.80:  Right.\n",
            "A.82:  That's,  that's right.\n",
            "A.84:  Yeah.\n",
            "A.86:  Yeah and,\n",
            "A.88:  That's right.\n",
            "A.90:  Yeah, that's true.  And, um, um.  ),\n",
            "A.92:  Yeah.  I don't know what el, what more we could say about that,\n",
            "A.94:  I was, yeah, nice talking to you, and I guess we just hang up, huh?\n",
            "A.96:  Okay, alright.\n",
            "A.98:  Nice talking to you.  Byebye.\n",
            "A.2:  Hi. A.4:  Um, yeah, I would like to talk about how you dress for work, and, and, um, A.6:  Uhhuh. A.8:  And is, A.10:  Right, right.  Is there, is there, um, a, is there a, like a code of dress A.12:  Right. A.14:  Right. A.16:  Right, right.  And does it, does it change?  I guess, um, you can, can you A.18:  Right. A.20:  Yes . A.22:  Yeah, yeah, well i, it's, A.24:  that's right.  And it, A.26:  Yeah, and it was usually, uh, also, uh, where I was, it was, um, i, in A.28:  and it was, it was a casual office, there was no formal dress code, but, A.30:  just in case things would come up during the day,  A.32:  sometimes unexpected meetings or a client would come in and would want to A.34:  uh, other times it could be very casual.  If you knew you would be at a A.36:  Um. A.38:  Uh, well, I was, I was in a, uh, private consulting firm,  A.40:  so, um, and, uh, A.42:  and, uh, anyway, um, right now I, I'm not, I'm not there but , A.44:  but, anyway, um, and seas, really same, season\n",
            "  Hi.   Um, yeah, I would like to talk about how you dress for work, and, and, um,   Uhhuh.     Right, right.  Is there, is there, um, a, is there a, like a code of dress   Right.   Right.   Right, right.    Right.   Yes .   Yeah, yeah, well i, it's,   that's right.    Yeah, and it was usually, uh, also, uh, where I was, it was, um, i, in   and it was, it was a casual office, there was no formal dress code, but,   just in case things would come up during the day,    sometimes unexpected meetings or a client would come in and would want to   uh, other times it could be very casual.  If you knew you would be at a   Um.   Uh, well, I was, I was in a, uh, private consulting firm,    so, um, and, uh,   and, uh, anyway, um, right now I, I'm not, I'm not there but ,   but, anyway, um, and seas, really same, season to season, um, uh, it   weather, you know,   wear light clothing I guess, if it's hot out, but, um,   that's     that's true, and, and I think also that, um, the women actually have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misspellings and Punctuation\n",
        "\n",
        "This is data from a second language learner of English.\n",
        "\n",
        "Taken from The EF-Cambridge Open Language Database ([EFCAMDAT](https://ef-lab.mmll.cam.ac.uk/EFCAMDAT.html))\n",
        "  - Large open-access corpus of English learner essays\n",
        "  - Comprises submissions from students worldwide who attend an online EF school.\n",
        "  - Learners are assigned to proficiency levels based on their initial placement test results or through successful course progression.\n",
        "  - There are 16 proficiency levels, aligned with the Common European Framework of Reference for Languages (CEFR)\n",
        "  - This student is level 4"
      ],
      "metadata": {
        "id": "Tl3UG1YKUnt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's install a spell check library\n",
        "!pip install pyspellchecker\n",
        "\n",
        "# uses a Levenshtein Distance algorithm to find permutations within an edit distance\n",
        "# of 2 from the original word. It then compares all permutations (insertions, deletions,\n",
        "# replacements, and transpositions) to known words in a word frequency list.\n",
        "# Those words that are found more often in the frequency list are more likely the correct results."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0GQwYOUWqlj",
        "outputId": "86a288a9-8bec-40a7-ff80-814a8cf4bbd4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Misspellings\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker() #assign variable to spellchecker\n",
        "\n",
        "import spacy #import spacy. We will need it for tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open('misspellings_punct.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "print(text[:1000]) #there are misspellings and problems with punctuation.\n",
        "\n",
        "spacy_doc = nlp(text)\n",
        "\n",
        "misspelled = []\n",
        "correct_spelled = []\n",
        "\n",
        "for token in spacy_doc:\n",
        "  if token.is_space == False: #gets rid of large white spaces that are treated as tokens\n",
        "    #print(token.text)\n",
        "    if spell.unknown([token.text]):  #is the word misspelled?\n",
        "      misspelled.append(token.text) #append it\n",
        "      correct_spelled.append(spell.correction(token.text)) #what is the closest correct spelling?\n",
        "      # and append it\n",
        "\n",
        "print(misspelled) #note that these are not terribly accurate. Misspelling is a hard task!\n",
        "# it misses cheep (context)\n",
        "print(correct_spelled)\n",
        "\n",
        "# replace the words using spacy\n",
        "cleaned_words = [] #list for all the cleaned words\n",
        "for token in spacy_doc:\n",
        "    if token.text in misspelled: #if token is \"misspelled\"\n",
        "        cleaned_words.append(correct_spelled[misspelled.index(token.text)] + token.whitespace_)\n",
        "        # replace it with \"correctly spelled word\" plus trailing whitespace if token is followed by whitespace\n",
        "    else:\n",
        "        cleaned_words.append(token.text + token.whitespace_) #if not, just include normal word\n",
        "\n",
        "# join words in list into a string\n",
        "cleaned_text = \"\".join(cleaned_words)\n",
        "print(cleaned_text) #this also takes care of punctuation problems because spaCy recognizes punctuation!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJm1dNVHWBJr",
        "outputId": "735fa876-11f8-4240-cf2c-8416193dee18"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear mom     I am going to go to a music festival this weekend. The ticket is cheep, only 20 yuan. I will see so many singers in the festival.I am going to listen to pop music.I will go camping.I am taking a tent ,T-shirt,sleeping bag and flashlight.I am also taking a umbralla. Maybe it will rain in sunday.         best regards        liji yuan\n",
            "['umbralla', 'liji']\n",
            "['umbrella', 'fiji']\n",
            "Dear mom     I am going to go to a music festival this weekend. The ticket is cheep, only 20 yuan. I will see so many singers in the festival.I am going to listen to pop music.I will go camping.I am taking a tent ,T-shirt,sleeping bag and flashlight.I am also taking a umbrella. Maybe it will rain in sunday.         best regards        fiji yuan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lowercasing\n",
        "\n",
        "We want all of our data to be in lowercase so it is standardized.\n",
        "\n",
        "This is simple to do in spaCy"
      ],
      "metadata": {
        "id": "SIj9jbbhFhjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"It should be pretty easy to put CAPITAL LETTER WORDS into lowercase using spaCy\"\n",
        "\n",
        "# Using only spaCy\n",
        "\n",
        "spacy_doc = nlp(text)\n",
        "\n",
        "for token in spacy_doc:\n",
        "    #lowercased_token = token.text.lower() #base Python\n",
        "    lowercased_token = token.lower_ #spaCy lowercase\n",
        "    print(lowercased_token)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmyX44E6FqPq",
        "outputId": "49ef8872-f774-40e3-e953-50ee7dedfcbd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "should\n",
            "be\n",
            "pretty\n",
            "easy\n",
            "to\n",
            "put\n",
            "capital\n",
            "letter\n",
            "words\n",
            "into\n",
            "lowercase\n",
            "using\n",
            "spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also just lowercase the text and then spaCy it\n",
        "# but spaCy tagger may work better on the original text, this is probably bad practice\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "spacy_doc = nlp(text)\n",
        "\n",
        "for token in spacy_doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlyO7-GiJrds",
        "outputId": "100afbb3-aae1-45ed-f152-169fab760b72"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it\n",
            "should\n",
            "be\n",
            "pretty\n",
            "easy\n",
            "to\n",
            "put\n",
            "capital\n",
            "letter\n",
            "words\n",
            "into\n",
            "lowercase\n",
            "using\n",
            "spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APPENDIX 1**\n",
        "\n",
        "Remember when we removed simple linguistic XML items?\n",
        "  - But maybe we want to use these annotations as part of our analysis?\n",
        "  - We can construct a map of character indexes between two string versions\n",
        "\n",
        "We can add them as token annotations in SpaCy.\n",
        "  - See below for code\n",
        "\n",
        "For simplicity, we will just attach each annotation to the preceding token, even though some of these describe phrasal issues.\n",
        "  - For example, `<SF>` indicates a sentence fragment at the end of the document."
      ],
      "metadata": {
        "id": "49OaWlZCNiZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn\n",
        "\n",
        "We have three pieces of data that need to be cleaned.\n",
        "\n",
        "1. childes.txt\n",
        "\n",
        "This comes from the Child Language Data Exchange System (CHILDES)\n",
        "  - established in 1984\n",
        "  - a central repository for data of first language acquisition\n",
        "  - earliest transcripts date from the 1960s, and as of 2015 has contents (transcripts, audio, and video) in 26 languages from 230 different corpora\n",
        "  - tanscriptions are coded in the CHAT (Codes for the Human Analysis of Transcripts) transcription format\n",
        "      - provides a standardized format for producing conversational transcripts.\n",
        "      - system also has options for phonological and morphological analysis\n",
        "\n",
        "CHAT\n",
        "  - Introductory texts\n",
        "  - Speaker information\n",
        "  - *MOT:\tMommy sit down at the table with you ?\n",
        "    - Utterance begins with *\n",
        "  - %mor:\tn:prop|Mommy v|sit n|down prep|at det|the n|table prep|with pro|you ?\n",
        "    - POS tags begin with %\n",
        "  - %gpx:\tlooks at Wanda\n",
        "    - Non-verbal begin with %gpx\n",
        "  - %act:\tsits facing Wanda\n",
        "    - Action begin with %act\n",
        "  - () unpronounced sounds\n",
        "  - xxx unknown words\n",
        "  - \\# pauses\n",
        "  - \\[ \\] unknown words\n",
        "\n",
        "2. tweet.txt\n",
        "  - Tweets are filled with lots of noise\n",
        "\n",
        "      - Emoticons\n",
        "      - @'s\n",
        "      - html links\n",
        "      - hash tags\n",
        "\n",
        "\n",
        "In groups, choose one and clean up the text!"
      ],
      "metadata": {
        "id": "JKiFQXajJuwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.tokens import Token, Doc\n",
        "\n",
        "Token.set_extension('annotations', default=[], force=True)\n",
        "\n",
        "def add_annotations(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    clean_text = ''\n",
        "\n",
        "    to_remove = [] # This is an array of all the character indexes corresponding to annotations\n",
        "    orig_starts = {} # This is a dictionary of annotations keyed by starting position in the orig text\n",
        "\n",
        "   # This pattern uses a slightly different strategy by collecting all characters that are not \">\", then finding the closing right-angle bracket.\n",
        "    for match in re.finditer(r' <[^>]+>', text):\n",
        "      to_remove.extend(range(match.start(), match.end()))\n",
        "      orig_starts[match.start()] = match.group()\n",
        "\n",
        "    # Remove tags while creating a mapping from original to cleaned text positions\n",
        "    orig_to_clean = {}\n",
        "    clean_pos = 0\n",
        "    for i, char in enumerate(text):\n",
        "        if not i in to_remove:\n",
        "            clean_text += char\n",
        "            clean_pos += 1\n",
        "        else:\n",
        "            orig_to_clean[i] = clean_pos\n",
        "\n",
        "    # Create the SpaCy Doc on the clean text\n",
        "    doc = nlp(clean_text)\n",
        "\n",
        "    # Find and attach tags using position mapping\n",
        "    for start, tag in orig_starts.items():\n",
        "        clean_pos = orig_to_clean[start]\n",
        "        token_idx = len([t for t in doc if t.idx < clean_pos]) - 1\n",
        "        if token_idx >= 0:\n",
        "            doc[token_idx]._.annotations.append(tag.strip())\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Test\n",
        "doc = add_annotations(text)\n",
        "print(doc)\n",
        "for token in doc:\n",
        "   if token._.annotations:\n",
        "       print(f\"{token.text}: {token._.annotations}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O472nzCpScXE",
        "outputId": "3817d50c-57d7-4ddc-e1c7-5ff80a33583c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it should be pretty easy to put capital letter words into lowercase using spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_tweets(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove HTML links\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions (@'s)\n",
        "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove emoticons and other punctuation\n",
        "    text = re.sub(r'\\(@.*?\\)', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Load the existing file\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/DS_5780_spring_25/text_cleaning/tweet.txt'\n",
        "\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "  tweets = file.readlines()\n",
        "\n",
        "# Clean each tweet\n",
        "cleaned_tweets = [clean_tweets(tweet) for tweet in tweets]\n",
        "\n",
        "# Save the cleaned tweets to a new file\n",
        "cleaned_file_path = '/content/drive/MyDrive/Colab Notebooks/DS_5780_spring_25/text_cleaning/cleaned-tweet.txt'\n",
        "with open(cleaned_file_path, 'w') as cleaned_file:\n",
        "  cleaned_file.write(\"\\n\".join(cleaned_tweets))\n",
        "\n",
        "print(f\"Cleaned tweets have been saved to {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv93TP77XvdU",
        "outputId": "84de0960-04f2-4bab-bfc1-9a6300ed99d3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned tweets have been saved to /content/drive/MyDrive/Colab Notebooks/DS_5780_spring_25/text_cleaning/cleaned-tweet.txt\n"
          ]
        }
      ]
    }
  ]
}